<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="./style.css" />
    <title>MLJ: Day 9</title>
  </head>
  <body>
    <h1>Machine Learning with Jona</h1>
    <h2>Day 9</h2>
    <p class="subtitle">Classification</p>
    <p>
      We might not want to predict a value, but to classify a group of object
      into a finite number of classes. For this the logistic regression might be
      really usefull, yeilding the probability of something belonging to a
      class, and setting a classification threshold: classifing an object if the probability is higher than a certain value. 
      <b>Accuracy</b> is <b>not</b> a good metric, because there could be
      class imbalances like 1 negative in 1000 positives, and the model by
      guessing always positive would be 99.9% accurate, put it wouldn't be a
      great model. </br>
      Other metrics we can use are <b>Precision</b> and <b>Recall</b>:
      <dl>
        <dt>Precision</dt>
        <dd>(True Positives)/(All positive predictions)</dd>
        <dd>when the model said "positive", was it right, did it say it too often?</dd>
        <dd>This tends to <b>lower</b> classification threshold</dd>

        <dt>Recall</dt>
        <dd>(True positives)/(Actual Positives)</dd>
        <dd>how often did the model correctly identify positives? Did it miss any?</dd>
        <dd>This tends to <b>increase</b> classification threshold</dd>
        
      </dl>
    </p>
    <p class="subtitle">ROC Curve</p>
    <p>The <b>ROC Curve (receiver operatingcharacteristic curve) </b> shows the performance of a classification model on all classification thresholds. 
    It plots False Positive rate ( <b>FPR</b>, defined as FP/(FP+TN)) against True Positive rate ( <b>TPR</b>, , synonim for Recall, defined with the same formula  ). 
    If we increase the classification threshold both will go up.
    </p>
    <p>An efficient way to evaluate this curve is with the <b>AUC: area under ROC curve</b>. The area under the ROC curve gives the following:
    probability that the model ranks a random positive higher than a random negative, by ranking i mean giving it a probability of being positive.</p>
    <p>Logistic prediction models should be <b>unbiased</b>. So the average of prediction should be roughly equal to the average of observation.
    The prediction bias is their difference. Adding a <b>calibration layer</b> is a bad idea, you will end up relying on it and not fixing the model itself.
    To analyze the bias use examples grouped in <b>buckets</b>, since the output of the model will be between 0 and 1 but each example is either 0 or 1.</p>

  </body>
</html>
