<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="./style.css" />
    <title>MLJ: Day 11</title>
    <script src="https://fred-wang.github.io/TeXZilla/TeXZilla-min.js"></script>
    <script src="https://fred-wang.github.io/TeXZilla/examples/customElement.js"></script>
  </head>
  <body>
    <h1>Machine Learning with Jona</h1>
    <h2>Day 11</h2>
    <p class="subtitle">Training Neural Nets</p>
    <p>
      Tensorflow handles the <em>backpropagation</em> on its own. But there are
      a number of ways in which it can go wrong
      <dl>
        <dt>Vanishing Gradients</dt>
        <dd>The gradient of the lower layers could get too close to zero, making the 
            training process very slow, or not event training at all. ReLU activation helps prevent this.</dd>
        <dt>Exploding Gradients</dt>
        <dd>On the contrary, gradients could also get very high, if the weights are very high, and the loss function
          may not converge. To avoid this normilizing the features helps.
        </dd>
        <dt>Dead ReLU Units</dt>
        <dd>Once the weighted sum gets below zero, a ReLU unit could get stuck, since it would always return 0.
          To avoid this lowering the Learning rate could be helpful.
        </dd>

      </dl>
    </p>
    <p class="subtitle"> Dropout Regularization</p>
    <p><b>Dropout</b> is another form of regularization, and it consists of skipping one random unit of activation
    for a single gradient step. The more units dropped, the greater the regularization. Like all regularizations, 
    it helps preventing overfitting.</p>
    <p class="subtitle">Multi-Class Neural Nets</p>
    <p>In a Multi-Class Neural Net we can use several binary logistic regressions, where we train a binary classifier for 
    each possible outcome using <b>one-vs-all</b>. For instance, given a problem with 8 different classes, 8 recognizers might be trained, one giving 
    a positive output, and 7 giving negative. This becomes very inefficient when working with a larger number of possible classes,
    and a better way to handle this would be with a deep Neural Net, in which each output node represents a different class.
    
    </p>
    <p class="subtitle">SoftMax</p>
    <p>In a Multi-Class model we would like the outputs to be probabilities, thus floating point values between 0 and 1, and we'd like
      that they'd add up to one. <b>SoftMax</b> equation does exactly that, it is implemented one layer before the output, and the formula is: <br>
      <la-tex>p(y= j|x) = \frac{e^{(w_j^T x +b_j)}}{\sum_{k \in K}e^{(w_k^T x +b_k)} }</la-tex> <br>
      Softmax is useful only in cases where each candidate can be a part of <b>only one class</b>, but if it has many possible classes it can belong to
      simultaneously then the alternatives are: to <b>not</b> use SoftMax and to use <b>multiple regression models</b> instead.
    </p>
    <p class="subtitle">SoftMax Options</p>
    <p>
      <dl>
        <dt>Full SoftMax</dt>
        <dd>SoftMax calculates probability for every possible class</dd>
        <dt>Candidate Sampling</dt>
        <dd>Calculates probabilities for all possible positive labels, and only for a random sample of negative labels.</dd>
      </dl>
      Candidate Sampling can improve performance when the number of classes climbs.
    </p>

  </body>
</html>
