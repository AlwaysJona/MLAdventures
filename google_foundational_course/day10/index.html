<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="./style.css" />
    <title>MLJ: Day 10</title>
    <script src="https://fred-wang.github.io/TeXZilla/TeXZilla-min.js"></script>
    <script src="https://fred-wang.github.io/TeXZilla/examples/customElement.js"></script>
  </head>
  <body>
    <h1>Machine Learning with Jona</h1>
    <h2>Day 10</h2>
    <p class="subtitle">Regularization: Sparsity</p>
    <p>
      If we have features that have the possibility of being a lot of different
      things, so they have very big dimensions, crossing them with other
      features of the same kind would produce a ton of weights, even when most
      of those combinations are irrelevant. Features of this kind are called
      <b>sparse</b>, and to take care of them we could use L<sub>1</sub>
      regularization. It consists of
      <b> minimizing the sum of the absolute values of the weights</b>. This
      way, irrelevant weights can reach 0, unlike L<sub>2</sub>
      regularization.
    </p>
    <p>
      in a model with a lot of informative features, the L<sub>1</sub>, may
      cause some of them to be exactly 0 in these cases:
        <dl>
          <dd>weakly informative features</dd>
          <dd>informative features strongly correlated to other informative features</dd>
          <dd>strongly informative features on different scales</dd>
        </dl>
    </p>
    <p class="subtitle">Neural Networks</p>
    <p>If a model is not linear, instead of trying to figure out the right non-linearity, 
      in order to make a feature cross that will make our linear model work, we can add hidden layers and at the end of each of them,
      pass a non linear function, called an <b>Activation Function</b>.

    </p>
    <p class="subtitle">Common Activation functions</p>
    <p> 
      To convert any value in a range from 0 to 1, the <b>sigmoid</b> is used:
      <la-tex> F(x) = \frac{1}{1+e^{-x}} </la-tex>. </br>
      A more commonly used Activation function is the <b>ReLU</b> (rectified linear unit): <la-tex>F(x) = max(0,x)</la-tex>
    </p>
    <p>Making a deep neural network with a lot of layers and nodes, will make it learn very complex shapes,
      possibly overfitting the data.
    </p>
    
  </body>
</html>
