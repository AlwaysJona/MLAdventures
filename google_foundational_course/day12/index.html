<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="./style.css" />
    <title>MLJ: Day 12</title>
    <script src="https://fred-wang.github.io/TeXZilla/TeXZilla-min.js"></script>
    <script src="https://fred-wang.github.io/TeXZilla/examples/customElement.js"></script>
  </head>
  <body>
    <h1>Machine Learning with Jona</h1>
    <h2>Day 12</h2>
    <p class="subtitle">Embeddings</p>
    <p>
      Embeddings are a type of hidden layers used to map the training data to
      lower dimensional, which means that each sample becomes a point in this
      space, and the closer two points are, the more similar the two samples
      are. This is useful for example for movie reccomendation in a streaming
      app. For this purpose <b>Collaborative Filtering</b> is used, which is the
      task of making prediction for one user's interest based on the other
      users' interests.
    </p>
    <h3>Categorical Input Data</h3>
    <p>
      Categorical data refers to the input features that represent one or more
      discrete items from a set of choices. For example the movies a user has
      watched. These are represented via <b>sparse tensors</b>, which are
      tensors with very few non-zero elements, so if we give a unique id to each
      movie, a user can be described by a sparse tensor of the movie he has
      watched.
    </p>
    <p>
      In order to represent such system we could use <em>one-hot encoding</em>,
      where for each user we have a tensor for dimension equal to the elements
      in the vocabulary (meaning all the movies in this case), and assign a 1 to
      the index corresponding to the movie the user has watched, and 0 othewise.
      This method generates very sparse input vectors, and it may lead to some
      problems.
    </p>
    <h4>Size of Network</h4>
    <p>
      Huge input vectors mean even larger amounts of weights for a neural
      Network, which means that more data will be neded to train the model
      properly. Furthermore computing and adjusting all these weights requires a
      lot of computation.
    </p>
    <h4>Lack of Meaningful relation between vectors</h4>
    <p>
      At this point a large sparse vector of the movie "Avengers" isn't any
      closer to a vector for the movie "Memento" then it is to "Shrek".
    </p>
    <h4>The solution: Embeddings</h4>
    <p>
      Embeddings allow us to translate these high dimension vectors into lower
      dimensional ones, maintaining the semantic relationships.
    </p>
    <h3>Embeddings: Translating to a Lower-Dimensional Space</h3>
    <p>
      Even a small multi dimensional space, can successfully group semantically
      similar items. Position, direction and distance can encode meaningful
      semantics in good embeddings. A useful embedding may be on the order of
      undreds of dimensions, which is probably several orders of magnitude
      smaller than the vocabulary.
    </p>
    <h3>Obtaining Embeddings</h3>
    <p>
      There are many techniques to capture the important structure of a high
      dimensional venctor, one of them is <b>PCA</b> (Principal Component
      Analysis), that tries to fined highly correlated embeddings, and collapse
      them into one.
      <h4>Word2vec</h4>
      <p>Word2vec is an algorithm developed at Google, that relies on the distributional hypotesis
        which states that words that often have the same neighboring words tend to have the same semantics.
        This algorithm trains a neural network to distinguish co-occourring groups of words from randomly grouped words.
        In one version the system makes a negative example by substituting a random nise word, for the target word.
        Given the positive "the plane flies", the system might swap to "the jogging flies". <br>
        The other version takes pairings containing the target word and random context words, and then learns to distinguish
        between positive cases and negative ones. <br>
        After the model has been trained, we have an embedding. So you can use the weights connecting the input layer
        to the hidden layer to map a sparse representation to a smaller vector.
      </p>
      <h3>Training an Embedding as Part of a Larger Model</h3>
      <p>You can also train the embeddings as part of the model, this way though it might take longer for the model to be optimized, so it
        may be better to train it separately. <br>
        In general when you have sparse data you can create an embedding unit that's a special type of hidden unit of size d,
        and it can be combined with any other features and hidden layers.
      </p>
    </p>
  </body>
</html>
